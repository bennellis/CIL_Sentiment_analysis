{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-15T14:14:14.718850Z",
     "start_time": "2025-05-15T14:14:04.662657Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from Hyperparameters.Embeddings.BertTokenEmbedder import BertTokenEmbedder\n",
    "from Hyperparameters.Dataloader.EmbeddingDataset import EmbeddingDataset\n",
    "from Hyperparameters.Dataloader.collate_fn import collate_fn\n",
    "from Hyperparameters.Models.BertPreTrainedClassifier import BertPreTrainedClassifier\n",
    "from Hyperparameters.Training.ActiveLearningLoop import active_learning_loop\n",
    "from Hyperparameters.Training.ActiveLearningLoop import query_entropy\n",
    "\n",
    "from Hyperparameters.Utils.Misc import get_device"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bruce\\anaconda3\\envs\\CIL\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering Model: SimpleModel (enforce_clean=False)\n",
      "Git Info:\n",
      "  User      : bbalfou <b_balfou@yahoo.com>\n",
      "  Commit    : 6c892749524f2f9246e08a85a69b7ee6f8028838\n",
      "  Branch    : bruce\n",
      "  File link : https://github.com/bennellis/CIL_Sentiment_analysis/blob/6c892749524f2f9246e08a85a69b7ee6f8028838/Hyperparameters\\Models\\ModelDummy.py\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:14:15.067304Z",
     "start_time": "2025-05-15T14:14:14.720268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name=\"FacebookAI/roberta-base\"\n",
    "csv_path=\"data/Sentiment/training.csv\"\n",
    "seed = 42\n",
    "\n",
    "lr = 1e-3\n",
    "class_order = [0,1,2]\n",
    "lr_top = 5e-5\n",
    "lr_mid = 3e-5\n",
    "lr_bot = 2e-5\n",
    "dropout = 0.4\n",
    "temperature = 0.5\n",
    "ce_weight = 0.1"
   ],
   "id": "3b81a937ebd486a7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:14:15.495953Z",
     "start_time": "2025-05-15T14:14:15.068311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(csv_path, index_col=0)\n",
    "label_map = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "df['label_encoded'] = df['label'].map(label_map)"
   ],
   "id": "872e944e9e7d9eb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:14:15.830984Z",
     "start_time": "2025-05-15T14:14:15.497552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['sentence'], df['label_encoded'],\n",
    "    stratify=df['label_encoded'], test_size=0.1, random_state=seed\n",
    ")"
   ],
   "id": "17586e5a239564b4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:14:15.835560Z",
     "start_time": "2025-05-15T14:14:15.832478Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "34654059ac29c202",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:14:30.101548Z",
     "start_time": "2025-05-15T14:14:15.837089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedder = BertTokenEmbedder(model_name)\n",
    "features = embedder.fit_transform(df['sentence'].to_list())\n",
    "labels = df['label_encoded'].to_numpy()"
   ],
   "id": "fb751f167363d54f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Processing batches:   0%|          | 0/3191 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b029d0722a94c24891d99ee1e5a8a39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:14:33.055071Z",
     "start_time": "2025-05-15T14:14:30.102729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "if embedder.is_variable_length:\n",
    "    feature_dataset = EmbeddingDataset(features, labels)\n",
    "\n",
    "    cache_name= model_name.replace(\"/\", \"_\")\n",
    "    cache_path = \"cache/\" + cache_name\n",
    "    emb_dataset_path = cache_path + \"emb_dataset.pt\"\n",
    "\n",
    "\n",
    "    if os.path.exists(emb_dataset_path):\n",
    "        embedded_feature_dataset = torch.load(emb_dataset_path, weights_only=False)\n",
    "    else:\n",
    "        feature_dataloader = DataLoader(feature_dataset, batch_size=8,collate_fn=collate_fn)\n",
    "        embedded_feature_dataset = embedder.embed_dataset(feature_dataloader)\n",
    "        os.makedirs(\"cache\", exist_ok=True)\n",
    "        torch.save(embedded_feature_dataset, emb_dataset_path)\n",
    "\n",
    "else:\n",
    "    raise Exception(\"blaalalal\")"
   ],
   "id": "f2af48e3dc1723e6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:14:34.319006Z",
     "start_time": "2025-05-15T14:14:33.057201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = BertPreTrainedClassifier(\n",
    "    model_name = model_name,\n",
    "    lr = lr,\n",
    "    pt_lr_bot = lr_bot,\n",
    "    pt_lr_mid = lr_mid,\n",
    "    pt_lr_top = lr_top,\n",
    "    class_order = class_order,\n",
    "    ce_weight = ce_weight,\n",
    "    temperature = temperature,\n",
    "    frozen = True,\n",
    "    custom_ll = True\n",
    ")"
   ],
   "id": "acb864db267797bb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:19:31.458592Z",
     "start_time": "2025-05-15T14:19:30.885699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from Hyperparameters.Training.ActiveLearningLoop import active_learning_loop\n",
    "from Hyperparameters.Training.ActiveLearningLoop import query_entropy\n",
    "active_learning_loop(\n",
    "        model,\n",
    "        get_device(),\n",
    "        embedded_feature_dataset,\n",
    "        query_entropy,\n",
    "        max_rounds=1000,\n",
    "        query_batch_size=1000,\n",
    "        train_epochs_per_round=3,\n",
    "        initial_label_count=1000,\n",
    "        val_split=0.2,\n",
    "        batch_size=16\n",
    ")"
   ],
   "id": "420cf56807ad9e78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using device: NVIDIA GeForce RTX 3080\n",
      "\n",
      "🔁 Round 1/1000 — Labeled: 1000\n",
      "Training BertPreTrainedClassifier on cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training:   0%|          | 0/32 [00:00<?, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "206d34cd1ef94288b06e192ae93cb0a3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mHyperparameters\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mTraining\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mActiveLearningLoop\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m active_learning_loop\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mHyperparameters\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mTraining\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mActiveLearningLoop\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m query_entropy\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m active_learning_loop(\n\u001B[32m      4\u001B[39m         model,\n\u001B[32m      5\u001B[39m         get_device(),\n\u001B[32m      6\u001B[39m         embedded_feature_dataset,\n\u001B[32m      7\u001B[39m         query_entropy,\n\u001B[32m      8\u001B[39m         max_rounds=\u001B[32m1000\u001B[39m,\n\u001B[32m      9\u001B[39m         query_batch_size=\u001B[32m1000\u001B[39m,\n\u001B[32m     10\u001B[39m         train_epochs_per_round=\u001B[32m3\u001B[39m,\n\u001B[32m     11\u001B[39m         initial_label_count=\u001B[32m1000\u001B[39m,\n\u001B[32m     12\u001B[39m         val_split=\u001B[32m0.2\u001B[39m,\n\u001B[32m     13\u001B[39m         batch_size=\u001B[32m32\u001B[39m\n\u001B[32m     14\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\CIL_Sentiment_analysis\\Hyperparameters\\Training\\ActiveLearningLoop.py:49\u001B[39m, in \u001B[36mactive_learning_loop\u001B[39m\u001B[34m(model, device, dataset, query_fn, max_rounds, query_batch_size, train_epochs_per_round, initial_label_count, val_split, batch_size)\u001B[39m\n\u001B[32m     46\u001B[39m train_subset = Subset(dataset, labeled_indices)\n\u001B[32m     47\u001B[39m train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m model.fit(train_loader, val_loader, epochs=train_epochs_per_round)\n\u001B[32m     52\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(pool_indices) == \u001B[32m0\u001B[39m:\n\u001B[32m     53\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m🎉 No more unlabeled samples.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\CIL_Sentiment_analysis\\Hyperparameters\\Models\\BaseModel.py:74\u001B[39m, in \u001B[36mBaseModel.fit\u001B[39m\u001B[34m(self, train_loader, val_loader, epochs, log_mlflow, plot_metrics)\u001B[39m\n\u001B[32m     71\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[32m     72\u001B[39m     \u001B[38;5;66;03m# Training\u001B[39;00m\n\u001B[32m     73\u001B[39m     \u001B[38;5;28mself\u001B[39m.train()\n\u001B[32m---> \u001B[39m\u001B[32m74\u001B[39m     train_loss, train_acc, train_neg_acc, train_nut_acc, train_pos_acc = \u001B[38;5;28mself\u001B[39m._run_epoch(train_loader, training=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     75\u001B[39m     train_losses.append(train_loss)\n\u001B[32m     76\u001B[39m     train_accs.append(train_acc)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\CIL_Sentiment_analysis\\Hyperparameters\\Models\\BaseModel.py:125\u001B[39m, in \u001B[36mBaseModel._run_epoch\u001B[39m\u001B[34m(self, data_loader, training)\u001B[39m\n\u001B[32m    121\u001B[39m pbar = tqdm(data_loader, desc=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33mTraining\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mif\u001B[39;00m\u001B[38;5;250m \u001B[39mtraining\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01melse\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[33m'\u001B[39m\u001B[33mEvaluating\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m,\n\u001B[32m    122\u001B[39m             unit=\u001B[33m'\u001B[39m\u001B[33mbatch\u001B[39m\u001B[33m'\u001B[39m, leave=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m pbar:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     x, y, kwargs = \u001B[38;5;28mself\u001B[39m._unpack_batch(batch)\n\u001B[32m    126\u001B[39m     x, y = x.to(\u001B[38;5;28mself\u001B[39m.device), y.to(\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m    128\u001B[39m     \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\CIL_Sentiment_analysis\\Hyperparameters\\Models\\BertPreTrainedClassifier.py:139\u001B[39m, in \u001B[36mBertPreTrainedClassifier._unpack_batch\u001B[39m\u001B[34m(self, batch)\u001B[39m\n\u001B[32m    135\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Unpack HF-formatted batch\"\"\"\u001B[39;00m\n\u001B[32m    137\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.frozen: \u001B[38;5;66;03m#If we've pre-computed the forward pass through the model, we just need to train the MLP\u001B[39;00m\n\u001B[32m    138\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m         batch[\u001B[32m0\u001B[39m],\n\u001B[32m    140\u001B[39m         batch[\u001B[32m2\u001B[39m],\n\u001B[32m    141\u001B[39m         {\u001B[33m'\u001B[39m\u001B[33mattention_mask\u001B[39m\u001B[33m'\u001B[39m: \u001B[38;5;28;01mNone\u001B[39;00m}\n\u001B[32m    142\u001B[39m     )\n\u001B[32m    143\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m: \u001B[38;5;66;03m#If we're doing the full pass, and inputs are tokens with an attention mask\u001B[39;00m\n\u001B[32m    144\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m    145\u001B[39m         batch[\u001B[32m0\u001B[39m][:, \u001B[32m0\u001B[39m].long(),\n\u001B[32m    146\u001B[39m         batch[\u001B[32m2\u001B[39m],\n\u001B[32m    147\u001B[39m         {\u001B[33m'\u001B[39m\u001B[33mattention_mask\u001B[39m\u001B[33m'\u001B[39m: batch[\u001B[32m0\u001B[39m][:, \u001B[32m1\u001B[39m].long().to(\u001B[38;5;28mself\u001B[39m.device)}\n\u001B[32m    148\u001B[39m     )\n",
      "\u001B[31mKeyError\u001B[39m: 0"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bb9bea80f8af3c80",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
