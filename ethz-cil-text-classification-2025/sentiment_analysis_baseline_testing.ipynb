{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqJLaVF2ag6Z"
   },
   "source": [
    "# Sentiment Classification Project"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r6qltM7-ahC-",
    "ExecuteTime": {
     "end_time": "2025-04-22T07:19:57.437939Z",
     "start_time": "2025-04-22T07:19:57.421876Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import custom_dataloader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import importlib\n",
    "import embeddings\n",
    "import models\n",
    "from sklearn.metrics import confusion_matrix\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZI8WpWMfcEPP"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "n-m747YTcAcq",
    "ExecuteTime": {
     "end_time": "2025-04-22T07:19:57.585959Z",
     "start_time": "2025-04-22T07:19:57.437939Z"
    }
   },
   "source": [
    "training_data = pd.read_csv('data/training.csv',index_col = 0)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WXIwO5cdcPsn",
    "ExecuteTime": {
     "end_time": "2025-04-22T07:19:57.665372Z",
     "start_time": "2025-04-22T07:19:57.649429Z"
    }
   },
   "source": [
    "# Encode the Labels (either as 0,1,2 for classification or -1,0,1 for regression)\n",
    "label_mapping = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "# label_mapping = {'negative': 1, 'neutral': 0, 'positive': 2}\n",
    "training_data['label_encoded'] = training_data['label'].map(label_mapping)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "AQpXm7wA_jIg",
    "outputId": "9823fa7d-c319-4efe-e4bb-a07f4d01aa27",
    "ExecuteTime": {
     "end_time": "2025-04-22T07:19:57.729148Z",
     "start_time": "2025-04-22T07:19:57.703221Z"
    }
   },
   "source": "# training_data\n",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cDLHIePxLbq"
   },
   "source": [
    "# Build Validation Set\n",
    "We use 90% of the reviews for training, and the remaining 10% for validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S2PkENpXxOtc",
    "ExecuteTime": {
     "end_time": "2025-04-22T07:19:57.798228Z",
     "start_time": "2025-04-22T07:19:57.785144Z"
    }
   },
   "source": [
    "\n",
    "sentences = training_data['sentence']\n",
    "labels = training_data['label_encoded']"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WtC5to6MyH9L",
    "ExecuteTime": {
     "end_time": "2025-04-22T07:19:57.876807Z",
     "start_time": "2025-04-22T07:19:57.808230Z"
    }
   },
   "source": [
    "# Fix Random Seed for Reproducibility\n",
    "random_seed = 42\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(sentences,labels, test_size=0.1, stratify=labels, random_state=random_seed)\n",
    "train_sentences, val_sentences = list(train_sentences), list(val_sentences)\n",
    "train_labels, val_labels = np.array(train_labels), np.array(val_labels)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T07:19:58.047309Z",
     "start_time": "2025-04-22T07:19:57.911922Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "# BALANCE_TRAINING = True\n",
    "\n",
    "# Combine sentences and labels for easy manipulation\n",
    "train_data = list(zip(train_sentences, train_labels))\n",
    "\n",
    "# Separate by class\n",
    "class_minus1 = [x for x in train_data if x[1] == -1]\n",
    "class_0 = [x for x in train_data if x[1] == 0]\n",
    "class_1 = [x for x in train_data if x[1] == 1]\n",
    "\n",
    "# Find the smallest class size\n",
    "min_size = min(len(class_minus1), len(class_0), len(class_1))\n",
    "\n",
    "# Undersample all classes to match the smallest class\n",
    "class_minus1_bal = resample(class_minus1, replace=False, n_samples=min_size, random_state=random_seed)\n",
    "class_0_bal = resample(class_0, replace=False, n_samples=min_size, random_state=random_seed)\n",
    "class_1_bal = resample(class_1, replace=False, n_samples=min_size, random_state=random_seed)\n",
    "\n",
    "# Combine and shuffle\n",
    "balanced_train_data = class_minus1_bal + class_0_bal + class_1_bal\n",
    "np.random.shuffle(balanced_train_data)\n",
    "\n",
    "# Split back into sentences and labels\n",
    "train_sentences_bal, train_labels_bal = zip(*balanced_train_data)\n",
    "train_sentences_bal, train_labels_bal = list(train_sentences_bal), np.array(train_labels_bal)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx2ynP3ZzI2W"
   },
   "source": [
    "# Bag-of-words + Logistic Regression baseline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bIIdOmcfzNcI",
    "ExecuteTime": {
     "end_time": "2025-04-22T07:20:17.657603Z",
     "start_time": "2025-04-22T07:19:58.156166Z"
    }
   },
   "source": [
    "importlib.reload(embeddings) # This is so when I make changes in embeddings it will reload the changes instead of caching the old file.\n",
    "\n",
    "# ****** change the following to access different embeddings\n",
    "\n",
    "# embedding_model = embeddings.BOWEmbedding(ngram_range=(1, 2), max_features=10000, stop_words=None, min_df=1, max_df=1.0)  #baseline\n",
    "# embedding_model = embeddings.BOWEmbedding(ngram_range=(1, 2), max_features=10000, stop_words='english', min_df=10, max_df=0.9)\n",
    "# embedding_model = embeddings.TFIDFEmbedding(ngram_range=(1, 2), max_features=10000, stop_words=None, min_df=1, max_df=1.0)\n",
    "# embedding_model = embeddings.BertPreTrained('bert-base-uncased') #bert-based-uncased model mean pooled\n",
    "# embedding_model = embeddings.BertPreTrainedClassifier('textattack/bert-base-uncased-yelp-polarity') #pre-trained bert yelp classifier model. Should have two classes (positive and negative sentiment)\n",
    "# embedding_model = embeddings.BertTokenEmbedder('textattack/bert-base-uncased-yelp-polarity') #pre-trained bert yelp classifier model tokenizer.\n",
    "# embedding_model = embeddings.BertTokenEmbedder('arpitk/product-review-sentiment-analyzer') #pre-trained bert product review sentiment classifier\n",
    "embedding_model = embeddings.BertTokenEmbedder('./models_pretrained/distilbert_product_855_04-21') # my fine-tuned model\n",
    "# embedding_model = embeddings.BertTokenEmbedder('distilbert/distilbert-base-cased') #pre-trained bert base cased\n",
    "# embedding_model = embeddings.Bert_variable_length() # This isn't working great for me\n",
    "# embedding_model = embeddings.Word2VecEmbedding2(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "\n",
    "# print(type(train_sentences_bal))\n",
    "\n",
    "n_samples = -1# set to -1 to get all samples\n",
    "n_val_samples = int(n_samples/10) if n_samples != -1 else -1\n",
    "X_train = embedding_model.fit_transform(train_sentences[:n_samples])#For quick testing with less data\n",
    "X_val = embedding_model.transform(val_sentences[:n_val_samples])\n",
    "\n",
    "Y_train = train_labels[:n_samples]\n",
    "Y_val = val_labels[:n_val_samples]"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 2872/2872 [00:17<00:00, 164.68it/s]\n",
      "Processing batches: 100%|██████████| 320/320 [00:01<00:00, 201.12it/s]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T07:20:17.724694Z",
     "start_time": "2025-04-22T07:20:17.703007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(X_train[1])\n",
    "# print(X_train.shape)\n",
    "# X_clipped = X_train[:,1:]\n",
    "# print(X_clipped.shape)\n",
    "# print(confusion_matrix(Y_train,np.argmax(X_clipped,axis=1)*-2+1, labels=[-1, 0, 1]))"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T07:20:17.772394Z",
     "start_time": "2025-04-22T07:20:17.756531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# number_examples = 5\n",
    "# label_map = {-1: 'negative', 0: 'neutral', 1: 'positive'}\n",
    "#\n",
    "# misclassified = [\n",
    "#     (label_map[true], label_map[pred], text)\n",
    "#     for true, pred, text in zip(Y_val, np.argmax(X_train,axis=1)*2-1, val_sentences)\n",
    "#     if true != pred and true!=0 and pred !=0\n",
    "# ]\n",
    "#\n",
    "# import random\n",
    "# for true, pred, text in random.sample(misclassified, number_examples):\n",
    "#     print(f\"True: {true}, Pred: {pred} → {text}\")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T07:20:17.825335Z",
     "start_time": "2025-04-22T07:20:17.803983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(type(train_sentences_bal[1]))\n",
    "# print(type(train_sentences_bal))\n",
    "# print(type(train_sentences))\n",
    "# print(type(val_labels))\n",
    "# print(type(val_labels[1]))\n",
    "# print(type(X_train))\n",
    "# print(type(X_val))\n",
    "# print(len(X_train[0]))\n",
    "# print(len(X_val[0]))\n",
    "# print(type(Y_train))\n",
    "# print(type(Y_val))"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T07:20:17.872982Z",
     "start_time": "2025-04-22T07:20:17.857236Z"
    }
   },
   "source": "# print(X_train[0])",
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T07:20:18.031642Z",
     "start_time": "2025-04-22T07:20:17.936590Z"
    }
   },
   "source": [
    "importlib.reload(custom_dataloader)\n",
    "\n",
    "#Initialize Dataloader If Necessary\n",
    "train_sampler = custom_dataloader.DynamicUnderSampler(Y_train, random_state=42)\n",
    "pre_compute = False\n",
    "if embedding_model.is_variable_length:\n",
    "    dataset_train = custom_dataloader.EmbeddingDataset(X_train, Y_train)\n",
    "    dataset_val = custom_dataloader.EmbeddingDataset(X_val, Y_val)\n",
    "    # print(len(dataset_train))\n",
    "    train_loader = DataLoader(\n",
    "        dataset_train,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=16,\n",
    "        collate_fn=custom_dataloader.collate_fn,\n",
    "    )\n",
    "    train_loader_pred = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=64,\n",
    "        collate_fn=custom_dataloader.collate_fn,\n",
    "    )\n",
    "    train_loader_pred_shuf = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_dataloader.collate_fn,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset_val,\n",
    "        batch_size=64,\n",
    "        collate_fn=custom_dataloader.collate_fn\n",
    "    )\n",
    "\n",
    "    if pre_compute:\n",
    "        emb_loader = embedding_model.precompute_embeddings(train_loader_pred)\n",
    "        emb_val_loader = embedding_model.precompute_embeddings(val_loader)\n",
    "\n",
    "else:\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(Y_val, dtype=torch.long)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor),sampler=train_sampler, batch_size=32)\n",
    "    train_loader_pred = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=32)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoRQiuRu0xfu",
    "outputId": "e04fe629-49f2-41b5-f3a3-e471ab565984",
    "ExecuteTime": {
     "end_time": "2025-04-22T07:20:18.047322Z",
     "start_time": "2025-04-22T07:20:18.031642Z"
    }
   },
   "source": [
    "\n",
    "# X_train\n",
    "# print(X_val.shape)\n",
    "# print(Y_val.shape)\n",
    "# print(Y_train)\n",
    "# for batch in train_loader:\n",
    "#     print(batch[0][0][0])\n",
    "#     print(batch[0][0][1])\n",
    "#     break\n",
    "    # print(batch['label'])\n",
    "    # break"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKoKqofM1tJ7"
   },
   "source": [
    "Now we train a logistic regression classifier..."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "id": "fAZgje2l1vpR",
    "outputId": "bff6da81-b9bd-421c-d40e-aeb493e8b24c",
    "ExecuteTime": {
     "end_time": "2025-04-22T07:23:29.663069Z",
     "start_time": "2025-04-22T07:23:29.305716Z"
    }
   },
   "source": [
    "importlib.reload(custom_dataloader)\n",
    "importlib.reload(models)\n",
    "# print(torch.cuda.memory_summary())# needed so it doesn't cache the functions\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "# print(torch.cuda.memory_summary())\n",
    "# print(X_train.shape)\n",
    "\n",
    "# ******** Change this to change the model\n",
    "# model = models.Logistic_Regression_Baseline() \n",
    "# model = models.LinearMLP(input_dim = X_val.shape[1])\n",
    "# model = models.NonLinearMLP(input_dim = X_val.shape[1])\n",
    "# model = models.DropoutMLP(input_dim = 768, lr = 0.00005, hidden_dim1= 256, hidden_dim2 = 64, dropout_prob = 0.2, ce_weight = 0.1)\n",
    "# model = models.BertPreTrainedClassifier('textattack/bert-base-uncased-yelp-polarity', lr = 2e-05, frozen = False) #pre-trained bert classifier\n",
    "# model = models.BertPreTrainedClassifier('arpitk/product-review-sentiment-analyzer', lr = 2e-05, frozen = False, class_order = [0,2,1]) #pre-trained distilbert product review classifier\n",
    "# model = models.BertPreTrainedClassifier('distilbert/distilbert-base-cased', lr = 1e-05, frozen = False, class_order = [0,1,2], dropout = 0.1, ce_weight = 0.1, temperature = 0.5) #pre-trained bert cased no fine tune\n",
    "model = models.BertPreTrainedClassifier('./models_pretrained/distilbert_product_855_04-21', lr = 1e-05, frozen = False, class_order = [0,2,1], dropout = 0.1, ce_weight = 0.1, temperature = 0.5) #my fine-tuned model\n",
    "# model = models.LSTMClassifier(input_dim = X_val[0].shape[1], lr=0.00005, dropout=0.2, hidden_dim=256, num_layers=2)\n",
    "\n",
    "\n",
    "\n",
    "# if model.is_variable_length:\n",
    "#     train_data = [{'text': text, 'label': label} for text, label in zip(train_sentences, train_labels)]\n",
    "#     val_data = [{'text': text, 'label': label} for text, label in zip(val_sentences, val_labels)]\n",
    "#     train_loader, val_loader = model.create_dataloaders(train_data, val_data)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#If model requires dataloader, Load data into dataloader:\n",
    "load_from_path = False\n",
    "if load_from_path:\n",
    "    model_path = \"models/distilbert_product_855_04-21.pt\"\n",
    "    print(\"Loading model from {}\".format(model_path))\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# print(X_train.shape)\n",
    "if model.use_dataloader:\n",
    "    if pre_compute:\n",
    "        model.fit(emb_loader,emb_val_loader,epochs=20)\n",
    "    else:\n",
    "        model.fit(train_loader,val_loader,epochs=20) #train_loader_pred_shuf has no class balancing\n",
    "    # model.fit(train_loader,epochs=10)\n",
    "else:\n",
    "    model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1ZrFZpoO10Jj",
    "ExecuteTime": {
     "end_time": "2025-04-22T07:24:23.376125Z",
     "start_time": "2025-04-22T07:23:31.438918Z"
    }
   },
   "source": [
    "if model.use_dataloader:\n",
    "    # Y_train_pred = model.predict(train_loader_pred)\n",
    "    # print(type(Y_train_pred))\n",
    "    # Predict on Validation Set\n",
    "    Y_val_pred = model.predict(val_loader)\n",
    "else: \n",
    "    # Y_train_pred = model.predict(X_train)\n",
    "    # Predict on Validation Set\n",
    "    Y_val_pred = model.predict(X_val)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predicting:   0%|          | 0/160 [00:00<?, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e21f812d66ca468f916aef833aa89d55"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "78F8MmkE2VwG",
    "ExecuteTime": {
     "end_time": "2025-04-22T07:25:52.075036Z",
     "start_time": "2025-04-22T07:25:52.050354Z"
    }
   },
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "# Score on Training Set\n",
    "# mae_train = mean_absolute_error(Y_train, Y_train_pred)\n",
    "# L_score_train = 0.5 * (2 - mae_train)\n",
    "# Score on Validation Set\n",
    "mae_val = mean_absolute_error(Y_val, Y_val_pred)\n",
    "L_score_val = 0.5 * (2 - mae_val)"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cJaoHn328Z6",
    "outputId": "4da98d93-92d1-48ab-feed-5b80d9e47abb",
    "ExecuteTime": {
     "end_time": "2025-04-22T07:25:53.733719Z",
     "start_time": "2025-04-22T07:25:53.718014Z"
    }
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# print(f'Evaluation Score (training set): {L_score_train:.05f}')\n",
    "print(f'Evaluation Score (validation set): {L_score_val:.05f}')\n",
    "\n",
    "\n",
    "# conf_matrix = confusion_matrix(Y_train,Y_train_pred, labels=[-1, 0, 1])\n",
    "# print(conf_matrix)\n",
    "conf_matrix = confusion_matrix(Y_val,Y_val_pred, labels=[-1, 0, 1])\n",
    "print(conf_matrix)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Score (validation set): 0.85454\n",
      "[[1488  440  262]\n",
      " [ 480 3856  579]\n",
      " [ 169  609 2326]]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T18:58:12.283583Z",
     "start_time": "2025-04-21T18:58:12.241101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model_path = \"distilbert_product_855_04-21\"\n",
    "torch.save(model.state_dict(), \"models/\" + model_path + \".pt\")\n",
    "model.model.save_pretrained(\"models_pretrained/\" + model_path)\n",
    "model.model.config.save_pretrained(\"configs/\" + model_path)\n",
    "model.tokenizer.save_pretrained(\"models_pretrained/\" + model_path)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('models_pretrained/distilbert_product_855_04-21\\\\tokenizer_config.json',\n",
       " 'models_pretrained/distilbert_product_855_04-21\\\\special_tokens_map.json',\n",
       " 'models_pretrained/distilbert_product_855_04-21\\\\vocab.txt',\n",
       " 'models_pretrained/distilbert_product_855_04-21\\\\added_tokens.json',\n",
       " 'models_pretrained/distilbert_product_855_04-21\\\\tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T15:02:48.356016Z",
     "start_time": "2025-04-21T15:02:48.334448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensemble:\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "#\n",
    "# model_paths = [\"models/bert_base_uncased_yelp_844_04-20.pt\",\n",
    "#                \"models/distilbert_cased_833_04-21.pt\",\n",
    "#                \"models/distilbert_cased_837_04-21.pt\",\n",
    "#                \"models/distilbert_cased_845_04-21.pt\",\n",
    "#                \"models/distilbert_product_849_04-20.pt\",\n",
    "#                \"models/distilbert_product_833_04-21.pt\",]\n",
    "# configs = [\"textattack/bert-base-uncased-yelp-polarity\",\n",
    "#            \"distilbert/distilbert-base-cased\",\n",
    "#            \"distilbert/distilbert-base-cased\",\n",
    "#            \"distilbert/distilbert-base-cased\",\n",
    "#            \"arpitk/product-review-sentiment-analyzer\",\n",
    "#            \"arpitk/product-review-sentiment-analyzer\"]\n",
    "#\n",
    "# for i in range(len(model_paths)):\n",
    "#     model_p = model_paths[i]\n",
    "#     model_c = configs[i]\n",
    "#     print(\"Loading model from {}\".format(model_p))\n",
    "#     model_ensemble = AutoModelForSequenceClassification.from_config(model_c)\n",
    "#     model.load_state_dict(torch.load(model_p))\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_6EjfvtdmIT"
   },
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V1iiA4ujdnPT",
    "ExecuteTime": {
     "end_time": "2025-04-21T11:14:36.677588Z",
     "start_time": "2025-04-21T11:14:36.619815Z"
    }
   },
   "source": "test_data = pd.read_csv('data/test.csv',index_col = 0)",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "0-98d8Ktd8u-",
    "outputId": "4c532496-beb7-4b71-e9ee-cda3df93789b",
    "ExecuteTime": {
     "end_time": "2025-04-21T06:18:33.833976Z",
     "start_time": "2025-04-21T06:18:33.818669Z"
    }
   },
   "source": "# test_data",
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V1cNuYsAdsYg",
    "ExecuteTime": {
     "end_time": "2025-04-21T11:14:40.542001Z",
     "start_time": "2025-04-21T11:14:38.665926Z"
    }
   },
   "source": [
    "X_test = embedding_model.transform(test_data['sentence'])\n",
    "\n",
    "\n",
    "\n",
    "# X_Custom_test = embedding_model.transform(np.array([\"if anyone wants to have the most delicious dinner, go somewhere else\"]))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 374/374 [00:01<00:00, 204.84it/s]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T06:18:50.742463Z",
     "start_time": "2025-04-21T06:18:50.726763Z"
    }
   },
   "cell_type": "code",
   "source": "# test_data['sentence']",
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qp5VgwqTeD2M",
    "ExecuteTime": {
     "end_time": "2025-04-21T11:15:43.313996Z",
     "start_time": "2025-04-21T11:14:42.917110Z"
    }
   },
   "source": [
    "# print(Y_train.shape)\n",
    "# print(X_test.shape)\n",
    "Y_test_fake_labels = np.ones(X_test.shape[0]) # This is just for the dataloader, too lazy to make it work without labels.\n",
    "# print(Y_test.shape)\n",
    "# Y_test = np.ones_like(X_test)\n",
    "\n",
    "if embedding_model.is_variable_length:\n",
    "    dataset_test = custom_dataloader.EmbeddingDataset(X_test, Y_test_fake_labels)\n",
    "    # print(len(dataset_train))\n",
    "    test_loader = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=64,\n",
    "        collate_fn=custom_dataloader.collate_fn,\n",
    "    )\n",
    "else:\n",
    "    if hasattr(X_test, 'toarray'):\n",
    "        X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
    "    else:\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, X_test_tensor), batch_size=32)\n",
    "\n",
    "# print(X_test[0].shape)\n",
    "\n",
    "if model.use_dataloader:\n",
    "    y_test = model.predict(test_loader)\n",
    "else: \n",
    "    y_test = model.predict(X_test)\n",
    "\n",
    "\n",
    "# print(test_data.index)\n",
    "\n",
    "# y_test = model.predict(X_Custom_test)\n",
    "# print(y_test)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predicting:   0%|          | 0/187 [00:00<?, ?batch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14128b6078e04dba99303c4f34a13309"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_rVrCrww-ZX",
    "outputId": "ec071f09-df37-481b-8b6d-4741f8f23be9",
    "ExecuteTime": {
     "end_time": "2025-04-21T11:15:43.629844Z",
     "start_time": "2025-04-21T11:15:43.598425Z"
    }
   },
   "source": [
    "# Save predictions in the correct format\n",
    "# y_test_adjusted = model._adjust_labels(y_test)\n",
    "y_labels = pd.Series(y_test).map({-1: 'negative', 0: 'neutral', 1: 'positive'})\n",
    "submission = pd.DataFrame({'id': test_data.index, 'label': y_labels})\n",
    "submission.to_csv('test_predictions.csv', index=False) # Update filename and path as needed\n",
    "print(\"Test predictions saved to 'test_predictions.csv'\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to 'test_predictions.csv'\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSCd2tL-4Eqj"
   },
   "source": [
    "# Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TvOtKWdpERea",
    "outputId": "2189bff2-0cf3-4582-fc43-e3ceab061d9a"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Bert_base_uncased' object has no attribute 'get_feature_names_out'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[239], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Top N most Important Words & Word Pairs per Output Class (Pos, Neutral, Negative)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m feature_names \u001B[38;5;241m=\u001B[39m \u001B[43membedding_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_feature_names_out\u001B[49m() \u001B[38;5;66;03m# get names of all tokens from vectorizer\u001B[39;00m\n\u001B[0;32m      3\u001B[0m coefs \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mcoef()  \u001B[38;5;66;03m# Weights per Feature for each Output Class; Shape: (Num_Output_Classes, Num_Features)\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Get Top_n Features by Weight for each Class\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Bert_base_uncased' object has no attribute 'get_feature_names_out'"
     ]
    }
   ],
   "source": [
    "# Top N most Important Words & Word Pairs per Output Class (Pos, Neutral, Negative)\n",
    "feature_names = embedding_model.get_feature_names_out() # get names of all tokens from vectorizer\n",
    "coefs = model.coef()  # Weights per Feature for each Output Class; Shape: (Num_Output_Classes, Num_Features)\n",
    "\n",
    "# Get Top_n Features by Weight for each Class\n",
    "def get_top_features(class_index, top_n=10):\n",
    "    class_coef = coefs[class_index]\n",
    "    top_indices = np.argsort(class_coef)[-top_n:]\n",
    "    return [feature_names[i] for i in reversed(top_indices)]\n",
    "\n",
    "print(\"Top words & bigrams for negative (-1):\", get_top_features(0))\n",
    "print(\"Top words & bigrams for positive (1):\", get_top_features(2))\n",
    "print(\"Top words & bigrams for neutral (0):\", get_top_features(1))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5YTJmQqEwEH",
    "outputId": "4f370302-ac76-4c3f-b873-cbeb0c74ece5",
    "ExecuteTime": {
     "end_time": "2025-04-21T06:21:05.454884Z",
     "start_time": "2025-04-21T06:21:05.438835Z"
    }
   },
   "source": [
    "# # Confusion Matrix - Negative, Neutral, Positive\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "#\n",
    "# conf_matrix = confusion_matrix(Y_val,Y_val_pred, labels=[-1, 0, 1])\n",
    "# print(conf_matrix)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5xVh6ZuHnzW",
    "outputId": "18f761fd-250d-46ab-b831-0ea0880e9697",
    "ExecuteTime": {
     "end_time": "2025-04-21T06:21:08.809092Z",
     "start_time": "2025-04-21T06:21:08.635520Z"
    }
   },
   "source": [
    "# Examples of Misclassified Sentences\n",
    "number_examples = 5\n",
    "label_map = {-1: 'negative', 0: 'neutral', 1: 'positive'}\n",
    "\n",
    "misclassified = [\n",
    "    (label_map[true.item()], label_map[pred.item()], text)\n",
    "    for true, pred, text in zip(Y_val, Y_val_pred, val_sentences)\n",
    "    if true != pred\n",
    "]\n",
    "\n",
    "import random\n",
    "for true, pred, text in random.sample(misclassified, number_examples):\n",
    "    print(f\"True: {true}, Pred: {pred} → {text}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: negative, Pred: neutral → I DONT SMOKE.\n",
      "True: positive, Pred: neutral → I think I'll stay here for another hour or so before heading home, and get some work completed.\n",
      "True: neutral, Pred: negative → Additionally, my groom booked a cabana on the adult side for the Friday before.\n",
      "True: neutral, Pred: positive → Forget their soda dispensers and yolo!!\n",
      "True: positive, Pred: neutral → I had bought their gift card from Staples that was on sale.\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
