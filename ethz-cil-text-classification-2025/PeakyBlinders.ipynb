{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/training.csv')\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "df.head()\n",
    "df = df[:1000] #for testing\n",
    "# df.describe()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings_batch(texts, model, tokenizer, batch_size=32):\n",
    "    \"\"\"\n",
    "    Get BERT embeddings for a batch of texts\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts to embed\n",
    "        model: Pretrained BERT model\n",
    "        tokenizer: BERT tokenizer\n",
    "        batch_size: Number of texts to process at once\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Embeddings for all input texts\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_embeddings = []\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=64\n",
    "        ).to(device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "    \n",
    "        embeddings = outputs.last_hidden_state\n",
    "        # print(embeddings.shape)\n",
    "        # embeddings = torch.mean(embeddings, dim=1) # this is to mean pool\n",
    "        \n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        # all_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "    # return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_labels(y, batch_size):\n",
    "    \"\"\"\n",
    "    Batches flat labels `y` into chunks of size `batch_size`.\n",
    "    \n",
    "    Args:\n",
    "        y: Flat tensor/list of labels (shape [N_total])\n",
    "        batch_size: Desired batch size\n",
    "    \n",
    "    Returns:\n",
    "        List of label batches (each shape [batch_size]), except possibly last\n",
    "    \"\"\"\n",
    "    if isinstance(y, list):\n",
    "        y = torch.tensor(y)\n",
    "    \n",
    "    # Split into batches\n",
    "    num_samples = len(y)\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size  # Ceiling division\n",
    "    \n",
    "    batched_y = []\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        batched_y.append(y[start:end])\n",
    "    \n",
    "    return batched_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:21<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer = CountVectorizer(ngram_range=(1,2), stop_words='english', min_df=20, max_features = 500)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_embed = BertModel.from_pretrained('bert-base-uncased')\n",
    "model_embed = model_embed.to(device)\n",
    "\n",
    "sentences = df['sentence'].tolist()  # Convert to list\n",
    "embeddings = get_bert_embeddings_batch(sentences, model_embed, tokenizer)\n",
    "\n",
    "# Add embeddings back to DataFrame (each embedding is a 768-dim vector)\n",
    "# embedding_columns = [f'bert_{i}' for i in range(768)]\n",
    "\n",
    "# def preprocess_text(sentences, tokenizer, max_length=64):\n",
    "#     encoded = tokenizer(\n",
    "#         sentences.tolist(),\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=max_length,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "#     return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "\n",
    "# input_ids, attention_mask = preprocess_text(df[\"sentence\"], tokenizer)\n",
    "\n",
    "\n",
    "# x = Vectorizer.fit_transform(df['sentence']).toarray()\n",
    "\n",
    "\n",
    "y = df['label'].values\n",
    "# y_batched = batch_labels(y, 32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(embeddings, y, test_size=0.5, random_state=0) \n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     embeddings,  # List of tensors (each is a batch)\n",
    "#     y_batched,          # Corresponding list of label tensors\n",
    "#     test_size=0.5,\n",
    "#     random_state=0,\n",
    "#     shuffle=True  # Shuffle the batches (not sequences within batches)\n",
    "# )\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# model = LogisticRegression(max_iter=1000, random_state=0)\n",
    "# model = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "# model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = SentimentDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = SentimentDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)  # 3 classes (negative, neutral, positive)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation for final layer (CrossEntropyLoss expects raw logits)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, output_dim=3):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Dimension of input features (768 for BERT embeddings)\n",
    "            hidden_dim: LSTM hidden state dimension\n",
    "            num_layers: Number of LSTM layers\n",
    "            output_dim: Number of classes (3 for sentiment)\n",
    "        \"\"\"\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,  # Set to True for bidirectional LSTM\n",
    "            dropout=0.2 if num_layers > 1 else 0  # Dropout between layers\n",
    "        )\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Optional: Initialize dropout\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # BERT embeddings come in as (batch_size, 768)\n",
    "        # LSTM expects (batch_size, seq_len, input_size)\n",
    "        # So we add a sequence length dimension of 1\n",
    "        # x = x.unsqueeze(1)  # Now shape (batch_size, 1, 768) This is only for when the input is not of sequences, but represents an embedding of the whole input.\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Get the final hidden state\n",
    "        # For multi-layer LSTM, we take the last layer's hidden state\n",
    "        final_hidden = hidden[-1]  # Shape (batch_size, hidden_dim)\n",
    "        \n",
    "        # Optional dropout\n",
    "        final_hidden = self.dropout(final_hidden)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(final_hidden)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Initialize the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (lstm): LSTM(768, 128, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (fc): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 768  # BERT base embedding dimension\n",
    "model = SentimentLSTM(input_dim=input_dim, hidden_dim=128, num_layers=2)\n",
    "# model = MLP(input_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Custom loss function for sentiment analysis.\n",
    "\n",
    "        Args:\n",
    "            y_pred: Predicted logits (before softmax), shape: (batch_size, num_classes)\n",
    "            y_true: True labels (LongTensor), shape: (batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            loss (Tensor): Loss value to minimize\n",
    "        \"\"\"\n",
    "        # Convert logits to probabilities\n",
    "        y_pred_prob = torch.softmax(y_pred, dim=1)  # Now differentiable\n",
    "\n",
    "        # Compute weighted sum of class indices for expected class prediction\n",
    "        y_pred_expected = torch.sum(y_pred_prob * torch.arange(y_pred.shape[1], device=y_pred.device), dim=1)\n",
    "\n",
    "        # Ensure y_true is float for computation\n",
    "        y_true = y_true.float()\n",
    "\n",
    "        # Compute Mean Absolute Error (MAE) on probabilities (differentiable)\n",
    "        mae = torch.mean(torch.abs(y_pred_expected - y_true))\n",
    "\n",
    "        # Compute the loss: 1 - custom score\n",
    "        loss = 1 - 0.5 * (2 - mae)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2600\n",
      "Epoch [2/5], Loss: 0.2533\n",
      "Epoch [3/5], Loss: 0.2504\n",
      "Epoch [4/5], Loss: 0.2323\n",
      "Epoch [5/5], Loss: 0.1927\n"
     ]
    }
   ],
   "source": [
    "criterion = CustomLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch) \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6507669102235107"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5460\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00       131\n",
      "     neutral       0.51      0.88      0.64       219\n",
      "    positive       0.66      0.54      0.59       150\n",
      "\n",
      "    accuracy                           0.55       500\n",
      "   macro avg       0.39      0.47      0.41       500\n",
      "weighted avg       0.42      0.55      0.46       500\n",
      "\n",
      "0.2503979839384556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\belli\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\belli\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\belli\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch) \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "print(total_loss/len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"sentiment_model.pth\") #save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:28<00:00, 13.18it/s]\n",
      "C:\\Users\\belli\\AppData\\Local\\Temp\\ipykernel_28716\\186951592.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"sentiment_model.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "sentences_inference = test_df['sentence'].tolist()\n",
    "x_inference = get_bert_embeddings_batch(sentences_inference, model_embed, tokenizer)\n",
    "\n",
    "# x_inference = Vectorizer.fit_transform(test_df['sentence']).toarray()\n",
    "X_inference_tensor = torch.tensor(x_inference, dtype=torch.float32).to(device)\n",
    "test_dataset = TensorDataset(X_inference_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "input_dim = x_inference.shape[1]\n",
    "model = MLP(input_dim)  # Ensure this matches your trained model\n",
    "model.load_state_dict(torch.load(\"sentiment_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        X_batch = batch[0].to(device)  # Extract the input batch\n",
    "        outputs = model(X_batch)  # Get logits\n",
    "        _, predicted_labels = torch.max(outputs, dim=1)  # Get predicted class indices\n",
    "\n",
    "        predictions.extend(predicted_labels.cpu().numpy())  # Convert to list\n",
    "\n",
    "label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "pred_labels = [label_map[p] for p in predictions]\n",
    "\n",
    "output_df = pd.DataFrame({\"id\": test_df[\"id\"], \"label\": pred_labels})\n",
    "output_df.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
